<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Making LDMs understand camera settings">
  <meta name="keywords" content="Camera Settings as Tokens, LDM, Stable Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models</title>

  <!-- Facebook Meta Tags -->
  <meta property="og:url" content="https://camera-settings-as-tokens.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models">
  <meta property="og:description" content="Camera Settings üì∑ + Text üìù ‚Æï Image">
  <meta property="og:image" content="https://camera-settings-as-tokens.github.io/static/videos/cat_cherry_blossom_trees_seed87.gif">

  <!-- Twitter Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="rich-text-to-image.github.io">
  <meta property="twitter:url" content="https://camera-settings-as-tokens.github.io/">
  <meta name="twitter:title" content="Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models">
  <meta name="twitter:description" content="Camera Settings üì∑ + Text üìù ‚Æï Image">
  <meta name="twitter:image" content="https://camera-settings-as-tokens.github.io/static/videos/cat_cherry_blossom_trees_seed87.gif">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-S6D6WB2SCD"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-S6D6WB2SCD');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
<!--   <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://site-assets.fontawesome.com/releases/v6.4.2/css/all.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Camera Settings as Tokens:<br>Modeling Photography on Latent Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            
              <!-- <a href="https://camera-settings-as-tokens.github.io">Anonymous Author</a><sup>1</sup>,</span> -->
              <!--Anonymous Authors</a></span> -->
              <span class="author-block">
              <a href="https://ishengfang.github.io/">I-Sheng Fang</a>,</span>
              <span class="author-block">
              <a href="https://odd2.github.io/Portfolio/">Yue-Hua Han</a>,</span>
              <span class="author-block">
              <a href="https://homepage.citi.sinica.edu.tw/pages/pullpull/">Jun-Cheng Chen</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Research Center of Information Technology Innovation, Academia Sinica</span>
          </div>

          <br>
          <div class="columns is-centered">
            <div class="is-size-4 publication-venue">
                SIGGRAPH Asia 2024
            </div>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://dl.acm.org/doi/10.1145/3680528.3687635"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block"> -->
<!--                 <a href="https://camera-settings-as-tokens.github.io" -->
<!--                    class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                       <i class="fab fa-youtube"></i> -->
<!--                   </span> -->
<!--                   <span>Video (Coming Soon)</span> -->
<!--                 </a> -->
<!--               </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/aiiu-lab/CameraSettings20K"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/aiiu-lab/Camera-Settings-as-Tokens"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/Camera-Settings-as-Tokens/Camera-Settings-as-Tokens"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-regular fa-face-smiling-hands"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/ishengfang/Camera-Settings-as-Tokens-SD2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-regular fa-face-smiling-hands"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>             
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <p class="title is-3"> TL;DR: Camera Settings üì∑ + Text üìù ‚Æï Image üñºÔ∏è </p>
        <img src="./static/images/teaser.png" width="100%">

      <p>
        Camera-settings-and-text to image generation. For the given camera settings and text prompt, our approach synthesizes the image based on these given conditions. 
        The camera settings are embedded as tokens in the text feature space of Latent Diffusion Models (LDMs) for more effective and precise image control.
      </p>

<!--       <iframe src="https://www.youtube.com/embed/1cmfAixwcmo?si=zVCqrhMa0uUX04Ai" 
        title="YouTube video player" 
        frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen
        style="
          max-width: 720px;
          width: 100%;
          aspect-ratio: 16/9;
              "></iframe> -->
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop"> 
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image models have revolutionized content creation, enabling users to generate images from natural language prompts. While recent advancements in conditioning these models offer more control over the generated results, photography‚Äîa significant artistic domain‚Äîremains inadequately integrated into these systems. Our research identifies critical gaps in modeling camera settings and photographic terms within text-to-image synthesis. Vision-language models (VLMs) like CLIP and OpenCLIP, which typically drive the text conditions through cross-attention mechanisms of conditional diffusion models, struggle to represent numerical data like camera settings effectively in their textual space. To address these challenges, we present CameraSettings20k, a new dataset aggregated from RAISE, DDPD, and PPR10K.Our curated dataset offers normalized camera settings for over 20,000 raw-format images, providing equivalent values standardized to a full-frame sensor. Furthermore, we introduce Camera Settings as Tokens, an embedding approach leveraging the LoRA adapter of Latent Diffusion Models (LDMs) to numerically control image generation based on photographic principles like focal length, aperture, film speed, and exposure time. Our experimental results demonstrate the effectiveness of the proposed approach to generate promising synthesized images obeying the photographic principles given the specified numerical camera settings. Furthermore, our work not only bridges the gap between camera settings and user-friendly photographic control in image synthesis but also sets the stage for future explorations into more physics-aware generative models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section> 

<section class="section">
  <!-- Introduction. -->
  <div class="container is-max-desktop">

      <div class="content has-text-centered">
      <h2 class="title is-3">Challenge <br> Stable Diffusion 1.5 with Numerical Camera Settings</h2>

        <img src="./static/images/failure_video_numerical_f16.png" width="54%">
      <p>
        <!-- The failure case of Stable Diffusion 2.0 with numerical camera settings. -->
        This image is generated with the text prompt "half body portrait of a beautiful Portuguese woman, pale skin, 
        brown hair with blonde highlights, wearing jeans, nature and cherry blossom trees in background with 16mm lens,
         <b>F/16</b> aperture, ISO 100, exposure time 0.01 second". 
      </p>
      </div>

    <p>
      Text-to-image models have revolutionized content creation, enabling users to generate images from natural language prompts. 
      While recent advancements in conditioning these models offer more control over the generated results, 
      photography‚Äîa significant artistic domain‚Äîremains inadequately integrated into these systems.
      For example, the image above does not reflect the camera settings in the text prompt. 
      With such a small aperture (<b>F/16</b>), the image should have a large depth of field, 
      but the generated image has a shallow depth of field.
    </p> 

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="container has-text-centered is-max-desktop">
      <h2 class="title is-3">Methods</h2>
      <p>

      </p>
     </div>

      <p>
        To address these challenges, we present <b>CameraSettings20k</b> and introduce <b>Camera Settings as Tokens</b>. 
      </p>
      <p>
        <b>CameraSettings20k</b> is a new dataset aggregated from RAISE, DDPD, and PPR10K datasets.
        Our curated dataset offers normalized camera settings for over 20,000 raw-format images, 
        providing equivalent values standardized to a full-frame sensor. 
      </p>

      <p>
        We introduce  <b>Camera Settings as Tokens</b>, an embedding approach leveraging the LoRA adapter of Latent Diffusion Models (LDMs) 
        to numerically control image generation based on photographic principles like focal length, aperture, film speed, and exposure time. 
      </p>
    
      <div class="column is-centered">
          <div class="content has-text-centered">
            <img src="./static/images/method.png" width="87%">
          <p>
            Our key idea is to embed camera settings as tokens in the text feature space of Latent Diffusion Models (LDMs) for more effective and precise image control.
          </p> 
        </div>
        </div>
      </div>
</section>

  

<section class="section">
  <div class="container is-max-desktop">
    <div class="container has-text-centered is-max-desktop">
      <h2 class="title is-2">Tasks</h2>
     </div>
     <!-- Image Editing -->
      <div class="content">
        <h2 class="title is-3">Image Editing</h2>
        <div class="container">
        <div class="juxtapose">
          <img src="./static/images/yann-lecun.jpg" data-label="Original" />
          <img src="./static/images/yann-lecun-no-bokeh.jpg" data-label="Edited" />
      </div>
        </div>
        <div class="container has-text-centered is-max-desktop">
        <p>
          Using SDEdit, our method can edit the depth of field of an image by 
          employing original camera settings as a negative prompt and target settings 
          as a positive prompt, enabling detailed background enhancement.
        </p>
      </div>
    </div>
      

      <!-- Fusion w/ ControlNet -->
      <div class="column is-centered">
        <div class="content">
          <h2 class="title is-3">Fusion with ControlNet</h2>
          <div class="content has-text-centered">
            <video poster="" id="cat32mm" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/cat_cherry_blossom_trees_seed87.mp4"
                      type="video/mp4">
            </video>

            <img src="./static/images/controlnet_cat.png" width="100%">
          </div>
          <p>
            Our method's U-Net LoRA for camera settings embedding can be fused with other models 
            like ControlNet, enabling image synthesis based on specified camera settings while 
            demonstrating compatibility and functionality.
          </p>
        </div>
      </div>
      <!--/ Fusion w/ ControlNet. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{fang2024camera,
      title={Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models},
      author={I-Sheng Fang and Yue-Hua Han and Jun-Cheng Chen},
      booktitle={SIGGRAPH Asia 2024 Conference Papers},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href=" ">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href=" " class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="static/js/juxtapose.js"></script>
<link rel="stylesheet" href="static/css/juxtapose.css">


</body>
</html>
